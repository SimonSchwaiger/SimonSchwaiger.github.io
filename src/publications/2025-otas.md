<h1 align="center">
OTAS: Open-vocabulary Token Alignment for Outdoor Segmentation
</h1>

<h3 align="center">
Simon Schwaiger<sup>1,2</sup>, Stefan Thalhammer<sup>2</sup>, Wilfried Wöber<sup>2</sup> and Gerald Steinbauer-Wagner<sup>1</sup>
</h3>

<i align="center">

This work was supported by the city of Vienna (MA23 – Economic Affairs, Labour and Statistics) through the project Stadt Wien Kompetenzteam für Drohnentechnik in der Fachhochschulausbildung (DrohnFH, MA23 project 35-02).

<sup>1</sup> Graz University of Technology, Faculty of Computer Science and Biomedical Engineering, Institute of Software Engineering and Artificial Intelligence, Inffeldgasse 16b/II, 8010 Graz, Austria

<sup>2</sup> University of Applied Sciences Technikum Wien, Faculty of Industrial Engineering, Research Group Digital Manufacturing, Automation and Robotics, 1200 Vienna, Austria

<a href="mailto:schwaige@technikum-wien.at">schwaige@technikum-wien.at</a>

</i>

<table align="center" style="border-collapse: collapse; max-width: 300pt;">
  <tr>
    <td align="middle" style="border: none;">
      <a href="https://github.com/SimonSchwaiger/SimonSchwaiger.github.io/tree/main/img/preprints/2025_otas_arxiv_version.pdf" style="color: white; font-size: 14pt;">
        <div style="background-color: #363636; border-radius: 50px; padding: 10px 20px; color: white; width: 80pt;">
            <img src="img/document_icon.png" height="14" style="transform:translate(-10%,-1px);"> Paper
        </div>
      </a>
    </td>
    <td align="middle" style="border: none;">
      <a href="https://github.com/SimonSchwaiger/SimonSchwaiger.github.io/tree/main/img/preprints/2025_otas_arxiv_version.pdf" style="color: white; font-size: 14pt;">
        <div style="background-color: #363636; border-radius: 50px; padding: 10px 20px; color: white; width: 80pt;">
            <img src="img/logo_github.png" height="14" style="transform:translate(-10%,-1px);"> Code
        </div>
      </a>
    </td>
    <td align="middle" style="border: none;">
      <a href="https://github.com/SimonSchwaiger/SimonSchwaiger.github.io/tree/main/img/preprints/2025_otas_arxiv_version.pdf" style="color: white; font-size: 14pt;">
        <div style="background-color: #363636; border-radius: 50px; padding: 10px 20px; color: white; width: 80pt;">
            <img src="img/logo_arxiv.png" height="14" style="transform:translate(-10%,-1px);"> arXiv
        </div>
      </a>
    </td>
  </tr>
</table>

<div style="max-width: 800px; margin: auto;", align="center">
  <img src="./img/otas_contrib_demo.gif" alt="OTAS Capability Overview GIF" width="48%">
  <img src="./img/otas_experiments_demo.gif" alt="OTAS Semantic Mapping Demo GIF" width="48%">
</div>

<h2 align="center"> Abstract</h2>

<i align="center">

Understanding open-world semantics is critical for robotic planning and control, particularly in unstructured outdoor environments. Current vision-language mapping approaches rely on object-centric segmentation priors, which often fail outdoors due to semantic ambiguities and indistinct semantic class boundaries. We propose OTAS—an Open-vocabulary Token Alignment method for Outdoor Segmentation. OTAS overcomes the limitations of open-vocabulary segmentation models by extracting semantic structure directly from the output tokens of pretrained vision models. By clustering semantically similar structures across single and multiple views and grounding them in language, OTAS reconstructs a geometrically consistent feature field that supports open-vocabulary segmentation queries. Our method operates zero-shot, without scene-specific fine-tuning, and runs at up to ≈17 fps. OTAS provides a minor IoU improvement over fine-tuned and open-vocabulary 2D segmentation methods on the Off-Road Freespace Detection dataset. Our model achieves up to a 151% IoU improvement over open-vocabulary mapping methods in 3D segmentation on TartanAir. Real-world reconstructions demonstrate OTAS’ applicability to robotic applications. The code and ROS node will be made publicly available upon paper acceptance.

</i>

***************************************

## Citation

If you use this work in your research, please cite our paper:

```bibtex
Coming Soon!
```